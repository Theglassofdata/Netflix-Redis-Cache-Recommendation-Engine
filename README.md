
# ğŸ¬ Netflix-Style Recommendation System Demo

## ğŸ§  System Architecture

<p align="center">
  <img src="ChatGPT Image Apr 15, 2025, 04_03_32 PM.png" alt="System Architecture" width="500"/>
</p>

This project showcases a real-time, containerized recommendation pipeline using Docker Compose.  
It integrates services like Spark, Flask API, Redis, Prometheus, and Grafana to simulate a Netflix-style recommendation system.

---

## âœ… Step 1: Show the System is Running

**ğŸ¤ Narration:**  
â€œIâ€™ve containerized and automated the entire Netflix-style recommendation pipeline using Docker Compose. Everything you see here is real-time and monitored.â€

**ğŸ”§ Command:**
```bash
docker ps
```

**ğŸ” Expected Output:**
```bash
CONTAINER ID   IMAGE            COMMAND                  STATUS          PORTS
abc123         spark            "/bin/bash"              Up 5 minutes    ...
def456         jupyter          "start-notebook.sh"      Up 5 minutes    ...
ghi789         flask-api        "python app.py"          Up 5 minutes    5000/tcp
jkl012         redis            "docker-entrypoint.sh"   Up 5 minutes    6379/tcp
mno345         prometheus       "/bin/prometheus"        Up 5 minutes    9090/tcp
pqr678         grafana          "/run.sh"                Up 5 minutes    3000/tcp
```

ğŸ–¼ï¸ *Screenshot Placeholder: Insert a screenshot of the terminal displaying the `docker ps` output.*

---

## âœ… Step 2: Simulate a User Watching a Movie

**ğŸ¤ Narration:**  
â€œLetâ€™s say user U999 watches C1010, which is, say, Stranger Things. The system automatically predicts and caches the next likely show.â€

**ğŸ”§ Command:**
```bash
curl -X POST http://localhost:5000/watched \
  -H "Content-Type: application/json" \
  -d '{"user_id": "U999", "content_id": "C1010"}'
```

**ğŸ” Expected Response:**
```json
{
  "user_id": "U999",
  "watched": "C1010",
  "predicted_next": "C1045"
}
```

ğŸ–¼ï¸ *Screenshot Placeholder: Insert a screenshot of the terminal displaying the curl response.*

---

## âœ… Step 3: Retrieve Cached Recommendation

**ğŸ¤ Narration:**  
â€œNow, the user comes back to the app and asks: what should I watch next? We instantly return the prediction from Redis.â€

**ğŸ”§ Command:**
```bash
curl http://localhost:5000/recommend/U999
```

**ğŸ” Expected Response:**
```json
{
  "user_id": "U999",
  "recommendation": "C1045",
  "cache_hit": true
}
```

ğŸ–¼ï¸ *Screenshot Placeholder: Insert a screenshot of the terminal displaying the curl response.*

---

## âœ… Step 4: Handle New User Scenario

**ğŸ¤ Narration:**  
â€œIf the user is new and we donâ€™t have predictions yet, the system handles it gracefully with a fallback.â€

**ğŸ”§ Command:**
```bash
curl http://localhost:5000/recommend/U1234
```

**ğŸ” Expected Response:**
```json
{
  "user_id": "U1234",
  "recommendation": "C000",
  "cache_hit": false,
  "fallback_used": true
}
```

ğŸ–¼ï¸ *Screenshot Placeholder: Insert a screenshot of the terminal displaying the curl response.*

---

## âœ… Step 5: Monitor System Metrics with Prometheus

**ğŸ¤ Narration:**  
â€œWe also expose Prometheus metrics for system health and activity.â€

**ğŸ”§ Access:**  
Navigate to [http://localhost:9090](http://localhost:9090)

**ğŸ” Metrics to Observe:**
- `cache_hits`
- `cache_misses`
- `fallbacks_used`

ğŸ–¼ï¸ *Screenshot Placeholder: Insert a screenshot of the Prometheus dashboard displaying the relevant metrics.*

---

## âœ… Step 6: Visualize Data with Grafana

**ğŸ¤ Narration:**  
â€œHereâ€™s the Grafana dashboard tracking live recommendations, cache efficiency, fallback trends, and more.â€

**ğŸ”§ Access:**  
Navigate to [http://localhost:3000](http://localhost:3000)

**ğŸ” Login Credentials:**
- Username: `admin`
- Password: `admin`

<p align="center">
  <img src="grafana.png" alt="System Architecture" width="500"/>
</p>*

---

## ğŸ Bonus Features

- Develop a simple HTML/JS frontend or CLI to simulate the watch and recommend flow.
- Display Spark job logs within JupyterLab or via Docker logs.
- Highlight the pipeline's extensibility for federated learning, edge caching, or A/B testing.
